{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitPartCount = 5\n",
    "splitSeed = 27\n",
    "hoursPerWindow = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.class_patient import Patients\n",
    "\n",
    "\n",
    "patients = Patients.loadPatients()\n",
    "len(patients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill measures whose null represent false value\n",
    "\n",
    "from constants import NULLABLE_MEASURES\n",
    "\n",
    "\n",
    "nullableMeasures = NULLABLE_MEASURES\n",
    "\n",
    "for measureName in nullableMeasures:\n",
    "    patients.fillMissingMeasureValue(measureName, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pco2 917\n",
      "ph 954\n",
      "po2 917\n",
      "albumin 406\n",
      "hba1c 326\n",
      "lymphocyte 446\n",
      "height 415\n",
      "urine-ketone 294\n",
      "crp 19\n"
     ]
    }
   ],
   "source": [
    "# remove measures with less than 80% of data\n",
    "\n",
    "measures = patients.getMeasures()\n",
    "\n",
    "for measure, count in measures.items():\n",
    "    if count < len(patients) * 80 / 100:\n",
    "        patients.removeMeasures([measure])\n",
    "        print(measure, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1206"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove patients with less than 80% of data\n",
    "\n",
    "patients.removePatientByMissingFeatures()\n",
    "len(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # remove patients with positive tag in first 12 hours\n",
    "\n",
    "from pandas import Timedelta\n",
    "\n",
    "\n",
    "patients.removePatientAkiEarly(Timedelta(hours=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total  1124\n",
      "AKI  392\n",
      "Ratio  0.3487544483985765\n"
     ]
    }
   ],
   "source": [
    "print(\"Total \", len(patients))\n",
    "print(\"AKI \", sum([1 for p in patients if p.akdPositive]))\n",
    "print(\"Ratio \", sum([1 for p in patients if p.akdPositive]) / len(patients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitedPatients = patients.split(splitPartCount, splitSeed)\n",
    "\n",
    "len(splitedPatients[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitedPatients = patients.split(splitPartCount, splitSeed)\n",
    "\n",
    "\n",
    "def trainTest():\n",
    "    for i in range(splitedPatients.__len__()):\n",
    "        testPatients = splitedPatients[i]\n",
    "\n",
    "        trainPatientsList = splitedPatients[:i] + splitedPatients[i + 1 :]\n",
    "        trainPatients = Patients(patients=[])\n",
    "        for trainPatientsElem in trainPatientsList:\n",
    "            trainPatients += trainPatientsElem\n",
    "\n",
    "        yield trainPatients, testPatients\n",
    "\n",
    "\n",
    "def trainValTest():\n",
    "    for i in range(splitedPatients.__len__()):\n",
    "        testPatients = splitedPatients[i]\n",
    "\n",
    "        trainPatientsList = splitedPatients[:i] + splitedPatients[i + 1 :]\n",
    "        trainPatients = Patients(patients=[])\n",
    "        for trainPatientsElem in trainPatientsList:\n",
    "            trainPatients += trainPatientsElem\n",
    "\n",
    "        *trainPatients, valPatients = trainPatients.split(5, 27)\n",
    "        tmpPatients = Patients(patients=[])\n",
    "        for trainPatientsElem in trainPatients:\n",
    "            tmpPatients += trainPatientsElem\n",
    "        trainPatients = tmpPatients\n",
    "\n",
    "        yield trainPatients, valPatients, testPatients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 225\n",
      "899 225\n",
      "899 225\n",
      "899 225\n",
      "900 224\n"
     ]
    }
   ],
   "source": [
    "for trainPatients, testPatients in trainTest():\n",
    "    print(len(trainPatients.patientList), len(testPatients.patientList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 23:08:34.194614: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-14 23:08:34.215643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += mask * -1e9\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask\n",
    "        )\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([Dense(dff, activation=\"relu\"), Dense(d_model)])\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        input_vocab_size,\n",
    "        maximum_position_encoding,\n",
    "        rate=0.1,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "        self.dropout = Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[tf.newaxis, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(tf.keras.layers.Layer):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_layers,\n",
    "#         d_model,\n",
    "#         num_heads,\n",
    "#         dff,\n",
    "#         target_vocab_size,\n",
    "#         maximum_position_encoding,\n",
    "#         rate=0.1,\n",
    "#     ):\n",
    "#         super(Decoder, self).__init__()\n",
    "\n",
    "#         self.d_model = d_model\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         self.embedding = Embedding(target_vocab_size, d_model)\n",
    "#         self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "#         self.dec_layers = [\n",
    "#             DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)\n",
    "#         ]\n",
    "#         self.dropout = Dropout(rate)\n",
    "\n",
    "#     def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "#         seq_len = tf.shape(x)[1]\n",
    "#         attention_weights = {}\n",
    "\n",
    "#         x = self.embedding(x)\n",
    "#         x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "#         x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "#         x = self.dropout(x, training=training)\n",
    "\n",
    "#         for i in range(self.num_layers):\n",
    "#             x, block1, block2 = self.dec_layers[i](\n",
    "#                 x, enc_output, training, look_ahead_mask, padding_mask\n",
    "#             )\n",
    "\n",
    "#             attention_weights[\"decoder_layer{}_block1\".format(i + 1)] = block1\n",
    "#             attention_weights[\"decoder_layer{}_block2\".format(i + 1)] = block2\n",
    "\n",
    "#         return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        dff,\n",
    "        input_vocab_size,\n",
    "        pe_input = 1,\n",
    "        rate=0.1,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        target_vocab_size = 1,\n",
    "        self.encoder = Encoder(\n",
    "            num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate\n",
    "        )\n",
    "\n",
    "        self.final_layer = Dense(target_vocab_size, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inp, training, enc_padding_mask=None):\n",
    "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(enc_output[:, 0, :]) # (batch_size, 1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    Concatenate,\n",
    "    Masking,\n",
    "    Conv1D,\n",
    "    MaxPooling1D,\n",
    "    BatchNormalization,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "# def createModel2(timeSteps, timeFeatures, staticFeatures):\n",
    "#     # time series layers\n",
    "#     timeInputLayer = Input(shape=(timeSteps, timeFeatures))\n",
    "#     maskingLayer = Masking(mask_value=0.0)(timeInputLayer)\n",
    "#     # cnnLayer = Conv1D(64, 3, activation=\"relu\", kernel_regularizer=l2(0.01))(maskingLayer)\n",
    "#     # batNormCnn = BatchNormalization()(cnnLayer)\n",
    "#     # poolingLayer = MaxPooling1D(2)(batNormCnn)\n",
    "#     seriesLayer = LSTM(64, return_sequences=True)(maskingLayer)\n",
    "#     seriesLayer2 = LSTM(64)(seriesLayer)\n",
    "#     seriesDense = Dense(32, activation=\"relu\")(seriesLayer2)\n",
    "\n",
    "\n",
    "#     # static layers\n",
    "#     staticInputLayer = Input(shape=(staticFeatures,))\n",
    "#     staticLayer = Dense(32, activation=\"relu\")(staticInputLayer)\n",
    "\n",
    "#     # combine layers\n",
    "#     combined = Concatenate(axis=1)([seriesDense, staticLayer])\n",
    "#     dense1 = Dense(16, activation=\"relu\")(combined)\n",
    "#     dropout1 = Dropout(0.5)(dense1)\n",
    "#     dense2 = Dense(1, activation=\"sigmoid\")(dropout1)\n",
    "\n",
    "#     model = Model(inputs=[timeInputLayer, staticInputLayer], outputs=dense2)\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=0.0001),\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[\"AUC\", \"accuracy\", \"precision\", \"recall\"],\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "def createTransformerModel(timeSteps, features):\n",
    "    model = Transformer(\n",
    "        num_layers=2,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        dff=2048,\n",
    "        input_vocab_size=features,\n",
    "        pe_input=timeSteps,\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"AUC\", \"accuracy\", \"precision\", \"recall\"],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(None, 12, 81), dtype=float32)\n  • training=True\n  • enc_padding_mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 113\u001b[0m\n\u001b[1;32m    109\u001b[0m npValX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([npValX, staticValX], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    112\u001b[0m trainY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(trainY)\n\u001b[0;32m--> 113\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnpTrainX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpValX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    125\u001b[0m testY \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(testY)\n",
      "File \u001b[0;32m~/codepy/hust.year2023.PredictingRiskDiabeticKetoacidosis-associatedKidneyInjury/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[34], line 22\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inp, training, enc_padding_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, training, enc_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(enc_output[:, \u001b[38;5;241m0\u001b[39m, :]) \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_output\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Transformer.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by Transformer.call():\n  • inp=tf.Tensor(shape=(None, 12, 81), dtype=float32)\n  • training=True\n  • enc_padding_mask=None"
     ]
    }
   ],
   "source": [
    "from utils.prepare_data import normalizeData, patientsToNumpy\n",
    "from constants import CATEGORICAL_MEASURES\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "loses = []\n",
    "aucs = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recals = []\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "models = []\n",
    "\n",
    "for i, (trainPatients, valPatients, testPatients) in enumerate(trainValTest()):\n",
    "    npTrainX, categoryEncoder, numericEncoder, oulier, columns = patientsToNumpy(\n",
    "        trainPatients, \n",
    "        hoursPerWindow,\n",
    "        CATEGORICAL_MEASURES,\n",
    "        timeSeriesOnly=True,\n",
    "        fromHour=0,\n",
    "        toHour=12,\n",
    "    )\n",
    "\n",
    "    npTestX, *_ = patientsToNumpy(\n",
    "        testPatients,\n",
    "        hoursPerWindow,\n",
    "        CATEGORICAL_MEASURES,\n",
    "        columns,\n",
    "        categoryEncoder,\n",
    "        numericEncoder,\n",
    "        oulier,\n",
    "        timeSeriesOnly=True,\n",
    "        fromHour=0,\n",
    "        toHour=12,\n",
    "    )\n",
    "\n",
    "    npValX, *_ = patientsToNumpy(\n",
    "        valPatients,\n",
    "        hoursPerWindow,\n",
    "        CATEGORICAL_MEASURES,\n",
    "        columns,\n",
    "        categoryEncoder,\n",
    "        numericEncoder,\n",
    "        oulier,\n",
    "        timeSeriesOnly=True,\n",
    "        fromHour=0,\n",
    "        toHour=12,\n",
    "    )\n",
    "\n",
    "    npTrainX = np.nan_to_num(npTrainX, nan=0)\n",
    "    npTestX = np.nan_to_num(npTestX, nan=0)\n",
    "    npValX = np.nan_to_num(npValX, nan=0)\n",
    "\n",
    "    ################### Static ###################\n",
    "    staticTrainX = trainPatients.getMeasuresBetween(measureTypes=\"static\")\n",
    "    staticTestX = testPatients.getMeasuresBetween(measureTypes=\"static\")\n",
    "    staticValX = valPatients.getMeasuresBetween(measureTypes=\"static\")\n",
    "\n",
    "    staticTrainX = staticTrainX.drop(columns=[\"subject_id\", \"hadm_id\", \"stay_id\", \"akd\"])\n",
    "    staticTestX = staticTestX.drop(columns=[\"subject_id\", \"hadm_id\", \"stay_id\", \"akd\"])\n",
    "    staticValX = staticValX.drop(columns=[\"subject_id\", \"hadm_id\", \"stay_id\", \"akd\"])\n",
    "\n",
    "    staticTrainX, staticTestX, staticValX = normalizeData(\n",
    "        staticTrainX, staticTestX, staticValX\n",
    "    )\n",
    "\n",
    "    staticLen = len(staticTrainX.columns)\n",
    "\n",
    "    staticTrainX = staticTrainX.to_numpy().astype(np.float32)\n",
    "    staticTestX = staticTestX.to_numpy().astype(np.float32)\n",
    "    staticValX = staticValX.to_numpy().astype(np.float32) # type: ignore\n",
    "\n",
    "    staticTrainX = np.nan_to_num(staticTrainX, nan=0)\n",
    "    staticTestX = np.nan_to_num(staticTestX, nan=0)\n",
    "\n",
    "    ################### labels ###################\n",
    "    trainY = [p.akdPositive for p in trainPatients]\n",
    "    testY = [p.akdPositive for p in testPatients]\n",
    "    valY = [p.akdPositive for p in valPatients]\n",
    "\n",
    "    # model = createModel2(npTrainX.shape[1], npTrainX.shape[2], staticLen)\n",
    "    model = createTransformerModel(npTrainX.shape[1], npTrainX.shape[2] + staticLen)\n",
    "\n",
    "    neg, pos = np.bincount(trainY)\n",
    "    weight0 = (1 / neg) * (len(trainY)) / 2.0\n",
    "    weight1 = (1 / pos) * (len(trainY)) / 2.0\n",
    "    weight = {0: weight0, 1: weight1}\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=50, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # expand static data to be time steps as time series data\n",
    "    staticTrainX = np.expand_dims(staticTrainX, axis=1)\n",
    "    staticTrainX = np.repeat(staticTrainX, npTrainX.shape[1], axis=1)\n",
    "    \n",
    "    staticTestX = np.expand_dims(staticTestX, axis=1)\n",
    "    staticTestX = np.repeat(staticTestX, npTestX.shape[1], axis=1)\n",
    "    \n",
    "    staticValX = np.expand_dims(staticValX, axis=1)\n",
    "    staticValX = np.repeat(staticValX, npValX.shape[1], axis=1)\n",
    "    \n",
    "    # append to npX\n",
    "    npTrainX = np.concatenate([npTrainX, staticTrainX], axis=2)\n",
    "    npTestX = np.concatenate([npTestX, staticTestX], axis=2)\n",
    "    npValX = np.concatenate([npValX, staticValX], axis=2)\n",
    "    \n",
    "\n",
    "    trainY = np.array(trainY)\n",
    "    history = model.fit(\n",
    "        npTrainX,\n",
    "        trainY,\n",
    "        epochs=1000,\n",
    "        batch_size=32,\n",
    "        validation_data=(npValX, np.array(valY)),\n",
    "        class_weight=weight,\n",
    "        callbacks=[early_stopping],\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    testY = np.array(testY)\n",
    "    loss, auc, accuracy, precison, recal = model.evaluate(npTestX, testY)\n",
    "\n",
    "    loses.append(loss)\n",
    "    aucs.append(auc)\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precison)\n",
    "    recals.append(recal)\n",
    "\n",
    "    train_loss_list.append(history.history['loss'])\n",
    "    val_loss_list.append(history.history['val_loss'])\n",
    "\n",
    "    pass\n",
    "\n",
    "print(\"Loses:\", loses, np.mean(loses), np.std(loses))\n",
    "print(\"AUCs:\", aucs, np.mean(aucs), np.std(aucs))\n",
    "print(\"Accuracies:\", accuracies, np.mean(accuracies), np.std(accuracies))\n",
    "print(\"Precisions:\", precisions, np.mean(precisions), np.std(precisions))\n",
    "print(\"Recals:\", recals, np.mean(recals), np.std(recals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Encoder.call().\n\n\u001b[1mDimensions must be equal, but are 81 and 12 for '{{node transformer_12_1/encoder_12_1/add}} = AddV2[T=DT_FLOAT](transformer_12_1/encoder_12_1/mul, transformer_12_1/encoder_12_1/strided_slice_1)' with input shapes: [?,12,81,512], [1,12,512].\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(None, 12, 81), dtype=float32)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m createTransformerModel(npTrainX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], npTrainX\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m staticLen)\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnpTrainX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpValX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codepy/hust.year2023.PredictingRiskDiabeticKetoacidosis-associatedKidneyInjury/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[47], line 22\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inp, training, enc_padding_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, training, enc_padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 22\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(enc_output[:, \u001b[38;5;241m0\u001b[39m, :]) \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_output\n",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m, in \u001b[0;36mEncoder.call\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     29\u001b[0m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39msqrt(tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model, tf\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m---> 30\u001b[0m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Encoder.call().\n\n\u001b[1mDimensions must be equal, but are 81 and 12 for '{{node transformer_12_1/encoder_12_1/add}} = AddV2[T=DT_FLOAT](transformer_12_1/encoder_12_1/mul, transformer_12_1/encoder_12_1/strided_slice_1)' with input shapes: [?,12,81,512], [1,12,512].\u001b[0m\n\nArguments received by Encoder.call():\n  • x=tf.Tensor(shape=(None, 12, 81), dtype=float32)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "model = createTransformerModel(npTrainX.shape[1], npTrainX.shape[2] + staticLen)\n",
    "\n",
    "history = model.fit(\n",
    "    npTrainX,\n",
    "    trainY,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    validation_data=(npValX, np.array(valY)),\n",
    "    class_weight=weight,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, train_loss in enumerate(train_loss_list):\n",
    "    plt.plot(\n",
    "        range(1, len(train_loss) + 1), train_loss, label=f\"Fold {i+1} Training Loss\"\n",
    "    )\n",
    "plt.title(\"Training Loss for Each Fold\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the validation loss for each fold\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, val_loss in enumerate(val_loss_list):\n",
    "    plt.plot(range(1, len(val_loss) + 1), val_loss, label=f\"Fold {i+1} Validation Loss\")\n",
    "plt.title(\"Validation Loss for Each Fold\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
