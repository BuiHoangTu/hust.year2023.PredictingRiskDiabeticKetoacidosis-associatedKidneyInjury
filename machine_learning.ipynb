{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from notebook_wrapper import data_selection\n",
    "\n",
    "dfData = data_selection.getNotebookOutput()\n",
    "dfData = dfData.loc[:, ~dfData.columns.str.contains(\"^Unnamed\")]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(dfData.dtypes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "sum(dfData[\"akd\"])/ len(dfData)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "droppingColumns = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "categoryColumns = [\"dka_type\", \"gender\", \"race\", \"liver_disease\", \"ckd_stage\"]\n",
    "labelColumn = \"akd\"\n",
    "numericColumns = [\n",
    "    col\n",
    "    for col in dfData.columns\n",
    "    if col not in droppingColumns + categoryColumns + [labelColumn]\n",
    "    and dfData[col].dtype != \"bool\"\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import auc, precision_score, roc_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, X, y):\n",
    "    train_sizes, train_scores, test_scores = learning_curve( # type: ignore\n",
    "        estimator, X, y, train_sizes=np.linspace(0.1, 1.0, 10), cv=5\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    plt.fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    plt.plot(train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\")\n",
    "    plt.plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_calibration_curve(estimator, X, y):\n",
    "    prob_pos = estimator.predict_proba(X)[:, 1]\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y, prob_pos, n_bins=10\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "    plt.plot([0, 1], [0, 1], \"--\", color=\"gray\", label=\"Perfectly calibrated\")\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Calibration Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classification_metric_evaluate(model, X, y, binary=True, Threshold=None):\n",
    "    # Predict probabilities\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    # For simplicity, let's assume the threshold is 0.5 if not provided\n",
    "    if Threshold is None:\n",
    "        Threshold = 0.5\n",
    "\n",
    "    # Convert probabilities to binary predictions based on the threshold\n",
    "    y_pred = (y_pred_proba >= Threshold).astype(int)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "\n",
    "    # Calculate confidence intervals for AUC\n",
    "    from scipy import stats\n",
    "\n",
    "    auc_values = np.array([auc for _, auc in sorted(zip(tpr, fpr))])  # Sort AUC values\n",
    "    auc_l, auc_u = stats.t.interval(\n",
    "        0.95, len(auc_values) - 1, loc=np.mean(auc_values), scale=stats.sem(auc_values)\n",
    "    )\n",
    "\n",
    "    # Construct dictionary to store evaluation metrics\n",
    "    metric_dic = {\n",
    "        \"AUC\": roc_auc,\n",
    "        \"AUC_L\": auc_l,\n",
    "        \"AUC_U\": auc_u,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"cutoff\": Threshold,  # Include the threshold in the dictionary\n",
    "    }\n",
    "\n",
    "    # Return evaluation metrics and ROC curve points\n",
    "    return fpr, tpr, metric_dic, y_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Drop the first 3 columns (ids)\n",
    "df = dfData.drop(columns=droppingColumns)\n",
    "\n",
    "# Encode categorical variables\n",
    "df = pd.get_dummies(df, columns=categoryColumns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df.drop(columns=[\"akd\"])\n",
    "y = df[\"akd\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "# fill missing value by k-nearest neighbors\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"distance\").fit(X_train)\n",
    "\n",
    "X_train_filled = pd.DataFrame(imputer.transform(X_train), columns=(X_train.columns))\n",
    "X_test_filled = pd.DataFrame(imputer.transform(X_test), columns=(X_test.columns))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "plot_learning_curve(clf, X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "clf.fit(X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "plot_calibration_curve(clf, X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "from sklearn import clone\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "best_auc = 0.0\n",
    "tprs_train, tprs_valid = [], []\n",
    "fpr_train_alls, tpr_train_alls = [], []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "KF = StratifiedKFold(n_splits=10, random_state=7, shuffle=True)\n",
    "for i, (train_index, valid_index) in enumerate(KF.split(X_train, y_train)):\n",
    "    X_train_kf, X_valid_kf = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    Y_train_kf, Y_valid_kf = y_train.iloc[train_index], y_train.iloc[valid_index]\n",
    "\n",
    "    model = clone(clf).fit(X_train_kf, Y_train_kf)\n",
    "\n",
    "    fpr_train, tpr_train, metric_dic_train, _ = classification_metric_evaluate(\n",
    "        model, X_train_kf, Y_train_kf,\n",
    "    )\n",
    "    fpr_valid, tpr_valid, metric_dic_valid, _ = classification_metric_evaluate(\n",
    "        model, X_valid_kf, Y_valid_kf, Threshold=metric_dic_train[\"cutoff\"]\n",
    "    )\n",
    "    metric_dic_valid.update({\"cutoff\": metric_dic_train[\"cutoff\"]})\n",
    "\n",
    "    if metric_dic_valid[\"AUC\"] > best_auc:\n",
    "        clf = model\n",
    "        resThreshold = metric_dic_train[\"cutoff\"]\n",
    "\n",
    "    tprs_valid.append(np.interp(mean_fpr, fpr_valid, tpr_valid))\n",
    "    tprs_valid[-1][0] = 0.0\n",
    "\n",
    "    # To draw a picture, you only need plt.plot(fpr,tpr). The variable roc_auc just records the value of auc and calculates it through the auc() function.\n",
    "    validationCurve =True\n",
    "    if validationCurve:\n",
    "        plt.plot(\n",
    "            fpr_valid, tpr_valid,\n",
    "            lw=1, alpha=0.4,\n",
    "            label='ROC fold %4d (auc=%0.3f 95%%CI (%0.3f-%0.3f))' % (\n",
    "            i + 1, metric_dic_valid['AUC'], metric_dic_valid['AUC_L'], metric_dic_valid['AUC_U']),\n",
    "        )\n",
    "\n",
    "    ##Training set ROC\n",
    "    fpr_train_alls.append(fpr_train)\n",
    "    tpr_train_alls.append(tpr_train)\n",
    "    tprs_train.append(np.interp(mean_fpr, fpr_train, tpr_train))\n",
    "    tprs_train[-1][0] = 0.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-val with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Define parameters for XGBoost\n",
    "params = {\n",
    "    # \"max_depth\": 9,  # maximum depth of the tree\n",
    "    # \"colsample_bylevel\": 1.0,\n",
    "    # \"colsample_bynode\": 0.0,\n",
    "    # \"colsample_bytree\": 0.23111437122462683,\n",
    "    # \"gamma\": 0.0,\n",
    "    # \"learning_rate\": 0.05,\n",
    "    # \"n_estimators\": 1000,  # Set the number of boosting rounds\n",
    "}\n",
    "\n",
    "classifier = xgb.XGBClassifier(**params)\n",
    "\n",
    "cv_scores = cross_val_score(classifier, X, y, cv=10, scoring=\"recall\")\n",
    "\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Calculate and print the mean accuracy\n",
    "mean_accuracy = cv_scores.mean()\n",
    "print(\"Mean accuracy:\", mean_accuracy)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Predicted Negative\", \"Predicted Positive\"],\n",
    "    yticklabels=[\"Actual Negative\", \"Actual Positive\"],\n",
    ")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score\n",
    "\n",
    "\n",
    "# Define your XGBoost model\n",
    "model = XGBClassifier()\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    \"max_depth\": [8, 9, 10],  # Maximum tree depth\n",
    "    \"learning_rate\": [0.1, 0.05, 0.01],  # Learning rate\n",
    "    \"n_estimators\": [500, 800, 100],  # Number of trees\n",
    "}\n",
    "\n",
    "# Define a custom scoring function to optimize for minimizing false negatives\n",
    "scorer = make_scorer(recall_score, pos_label=1)\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model, \n",
    "    param_grid=param_grid,\n",
    "    # scoring=scorer,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_params"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# Train the model with the best parameters\n",
    "best_model = XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(\"F1 Score:\", f1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "\n",
    "# Feature selection using LassoCV\n",
    "lasso_cv = LassoCV(cv=5, random_state=7)\n",
    "lasso_cv.fit(X, y)\n",
    "selected_features = X.columns[lasso_cv.coef_ != 0]\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    display(pd.DataFrame({\"column_name\": X.columns, \"lasso_coef\": lasso_cv.coef_}).T)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "# Splitting data into training and validation cohorts\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"XGBClassifier\": XGBClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LGBMClassifier\": LGBMClassifier(),\n",
    "    \"SVC\": SVC(),\n",
    "    \"MLPClassifier\": MLPClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "}\n",
    "\n",
    "# Perform cross-validation and evaluate each model\n",
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    scores = cross_val_score(clf, X, y, cv=10)\n",
    "    results[clf_name] = scores.mean()\n",
    "\n",
    "# Print results\n",
    "for clf_name, score in results.items():\n",
    "    print(f\"{clf_name}: {score}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "estimators = [\n",
    "    (\"encoder\", TargetEncoder()),\n",
    "    (\"clf\", xgb.XGBClassifier(random_state=8)),\n",
    "]\n",
    "\n",
    "pipe = Pipeline(steps=estimators, verbose=True)\n",
    "pipe"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "\n",
    "searchSpace = {\n",
    "    \"clf__max_depth\": Integer(5, 11),\n",
    "    \"clf__learning_rate\": Real(0.001, 1.0, prior=\"log-uniform\"),\n",
    "    \"clf__subsample\": Real(0.5, 1.0),\n",
    "    \"clf__colsample_bytree\": Real(0.0, 0.5),\n",
    "    \"clf__colsample_bylevel\": Real(0.5, 1.0),\n",
    "    \"clf__colsample_bynode\": Real(0.0, 0.5),\n",
    "    \"clf__reg_alpha\": Real(0.0, 10.0),\n",
    "    \"clf__reg_lambda\": Real(0.0, 10.0),\n",
    "    \"clf__gamma\": Real(0.0, 10.0),\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    pipe, search_spaces=searchSpace, cv=3, n_iter=50, scoring=\"roc_auc\", random_state=7\n",
    ")\n",
    "\n",
    "opt.fit(X, y)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "opt.best_estimator_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "opt.best_score_"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "opt.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "opt.best_estimator_.steps"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
